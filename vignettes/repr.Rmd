---
title: "Reproducible research concepts and tools, 2017"
author: "Vincent J. Carey, stvjc at channing.harvard.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Repr notes}
---

## Basic definitions

1) Analysis = software + data + environment + invocations

2) Reproducible analysis = analysis that can be carried out by independent parties

3) Extensible analysis = analysis supporting independent variations

4) Study = Design + implementation + analysis

5) Replicable study = A study that, when executed by independent parties,
produces statistically compatible interpretations

## Open questions

1) What is an environment for an analysis?

2) What are independent parties? (See Kahneman's etiquette for replication)

3) What are independent variations on an analysis and why are they important?

4) When are two interpretations of related or identical studies statistically compatible?  Reformulate in terms of the standards of replicability for a given field.  Example: GWAS catalog concept of replicated finding.


## The three themes underlying reproducibility research

1) providing code and data and environment to independent parties
to diminish risk of analyses that are not reproducible

2) fortifying criteria of statistical soundness of analyses (study interpretations) to control risk of non-replicability of studies

3) doing 1) and 2) in ways that are cost-effective



## Overview
### Key concepts
- Accessible versioned data
    - Authentication and validity
- Accessible versioned software
    - Reliability through continuous integration
- Workflows that produce key inferences

### Aims
- Reliability and efficiency in the lab
- Reliability and efficiency of dissemination, uptake, and extension
- Foster refinement of inferences, faithfully implement a vision of open science

## Accessible data
### Local access and management
- Protocol
- Self-description
- Discovery
- Evolution through revision

### Global access and management
- Cloud: democratize access to data
- Discovery problem
- "Move computation to the data"

## Versioned data
### "File" characteristics must be recorded and sufficiently rich
### Audit trail of changes to file contents
### Determine provenance of a file derived from another file
### Propagate durable information about data provenance
### More general data resources (distributed files, streams)

## Simple formulations of reproducibility condition are challenging
- Any computational research of the form (result = data + code) must
be reproducible
- But we have all encountered situations of the form: "This code used
to work!  i haven't changed anything!"  But either it fails to run or
it produces results that are not compatible with previous run.
- The environment has changed, or the contents of data or code have changed
to lead to the surprise.
- Can we avoid this?  Two basic responses: continuous integration with tests (you are told
immediately when the result is not recovered in a current run) or environment-preserving
containers (reproducibility truly guaranteed)

